experiment:
  name: "optml-main"
  seed: 0

paths:
  data: outputs/features_and_dh_returns.parquet

data:
  date_col: date
  group_col: date
  target: y_price
  moneyness_col: moneyness_std
  option_type_col: is_call
  features: []
  feature_groups:
    I:
      - moneyness_std
      - tau
      - is_call
      - delta_model
      - theta_model
      - gamma_model
      - vega_model
      - iv
      - log_mid_t
      - opt_rel_spread_final
      - opt_rel_spread_chg
      - baspread_chg
      # - volume_to_oi
      - log_open_interest_opt
      - d_log_open_interest_opt
    B:
      - atm_iv
      - atm_term_slope
      - smile_slope
      - convexity_proxy
      - rr25_proxy
      - bf25_proxy
      - ivrv_ratio_pred
      # - iv_prev_bucketed
      - d_iv_atm
      - atm_ivchg_proxy
      - dist_to_wall
      # - ddist_to_wall
      - gex_proxy
      # - dgex_proxy
      - ddist_to_wall_date
      - dgex_proxy_date
      # - oi_herf
      - oi_herf_date
      - log_pcr_oi_opt
      - d_log_pcr_oi_opt
    M:
      # - rv_pred
      - rv_chg
      - realskew_chg
      - fut_baspread_chg
      - fut_baspread_z
      - log_taker_buy_sell_ratio
      - log1p_taker_buy_volume
      - log1p_long_liquidations_usd
      - log1p_short_liquidations_usd
      - d_log_taker_buy_sell_ratio
      - d_log1p_taker_buy_volume
      - d_log1p_long_liquidations_usd
      - d_log1p_short_liquidations_usd
      - basis
      - d_basis
      - funding_rates
      # - d_funding_rates
      - log_estimated_leverage_ratio
      - d_log_estimated_leverage_ratio
      - log_open_interest
      - d_log_open_interest
      - log_pcr_oi
      - d_log_pcr_oi
      - bitw_mom_5_1
      - bitw_mom_10_1
      - bitw_mom_21_1
      # - bitw_mom_exbtc_10_1
      - bitw_vol_10
      # - rs_bitw_btc_10
    C:
      - log1p_SPOT_inflow_total
      - log1p_SPOT_outflow_total
      - asinh_SPOT_netflow_total
      - log1p_SPOT_reserve_usd
      - log1p_SPOT_transactions_count_inflow
      - log1p_SPOT_transactions_count_outflow
      # - log_SPOT_stablecoins_ratio_usd
      - logit_SPOT_exchange_whale_ratio
      - addresses_count_active
      - d_log1p_SPOT_inflow_total
      - d_log1p_SPOT_outflow_total
      - d_asinh_SPOT_netflow_total
      - d_log1p_SPOT_reserve_usd
      - d_log1p_SPOT_transactions_count_inflow
      - d_log1p_SPOT_transactions_count_outflow
      # - d_log_SPOT_stablecoins_ratio_usd
      - d_logit_SPOT_exchange_whale_ratio
    T:
      - z_gt
      - rtrs_pos_z
      - rtrs_neg_z
      - rtrs_neu_z
      - rtrs_total_z
      - reddit_pos_z
      - reddit_neg_z
      - reddit_neu_z
      - reddit_total_z
    INTERACTIONS:
      - d_funding_rates_x_mny
      - d_funding_rates_x_tau
      - d_log1p_SPOT_inflow_total_x_mny
      - d_log1p_SPOT_inflow_total_x_tau
      - d_log_open_interest_x_mny
      - d_log_open_interest_x_tau
      - d_log_pcr_oi_x_mny
      - d_log_pcr_oi_x_tau


  shift_features:
    enabled: false
    only_date_constant: true
    force_all: false

  exclusions_by_target:
    y_gamma: []
    y_vega: [price_per_vega]

  target_norm:
    kind: none
    abs: true
    floor: 1e-6
    # column: null
    # force: false

  target_guardrails:
    winsorize:
      enabled: true      # false to disable
      lower_q: 0.01
      upper_q: 0.99
      fit_on: train      # learn thresholds on train only
    transform:
      enabled: false     # true to enable
      kind: asinh        # asinh | log1p | none
      scale:
        method: mad

contracts:
  type: all
  moneyness: all
  atm_band: [-1.25, 1.25]

preprocess:
  linear_nn:
    min_non_null_frac: 0.60
    variance_threshold: 1e-12
    winsorize:
      lower_q: 0.005
      upper_q: 0.995
    impute: median
    yeo_johnson: true
    standardize: true
  trees:
    min_non_null_frac: 0.20
    variance_threshold: 1e-12
    winsorize:
      lower_q: 0.00
      upper_q: 1.00
    impute: median
    yeo_johnson: false
    standardize: false

split:
  scheme: time_ordered
  train_frac: 0.70
  val_frac: 0.15
  test_frac: 0.15
  purge_gap: 1

benchmark: zero

models:
  enable:
    linear:
      # - ols
      - ridge
      - lasso
      - elasticnet
      - pcr
      - pls
    nonlinear:
      - rf
      - lgbm_gbdt
      # - lgbm_gbdt_huber      # ← NEW Huber GBDT model
      - lgbm_dart
      # - lgbm_dart_huber      # ← NEW Huber DART model
      - ffn

tuning:
  strategy: random
  n_iter: 128
  keep_top_k: 2
  seed: 0
  build_family_ensemble: true
  cv:
    enabled: true
    kind: time_grouped
    n_splits: 3
    gap: 1
    use_train_plus_val: true
    min_train_size: 20
  spaces:
    ols: {}
    ridge:
      alpha:
        dist: loguniform
        low: 1e-6
        high: 10.0
      max_iter:
        values: [10000]
    lasso:
      alpha:
        dist: loguniform
        low: 1e-6
        high: 1.0
      max_iter:
        values: [10000]
    elasticnet:
      alpha:
        dist: loguniform
        low: 1e-6
        high: 1.0
      l1_ratio:
        dist: uniform
        low: 0.0
        high: 1.0
      max_iter:
        values: [10000]
    pcr:
      pca__n_components:
        values: [1, 2, 3, 4, 5, 6]
      ridge__alpha:
        values: [0.0001, 0.001, 0.01, 0.1, 1.0]
    pls:
      n_components:
        values: [1, 2, 3, 4, 5, 6]
    rf:
      n_estimators:
        values: [600, 1000]
      max_depth:
        values: [4, 6, 8]
      max_features:
        values: ["sqrt", "log2"]
      min_samples_leaf:
        values: [20, 50, 100]
      min_samples_split:
        values: [2, 5, 10]
      min_impurity_decrease:
        values: [0.0, 1e-7, 1e-6]
      bootstrap:
        values: [true]
      max_samples:
        values: [0.5, 0.7, 0.9]
    lgbm_gbdt:
      n_estimators:
        values: [600, 900, 1200]
      learning_rate:
        values: [0.005, 0.01, 0.02, 0.03]
      max_depth:
        values: [-1, 3, 4]
      num_leaves:
        values: [31, 63, 127]
      min_child_samples:
        values: [50, 100, 200]
      min_split_gain:
        values: [0.0, 0.05, 0.1]
      min_child_weight:
        values: [0.001, 0.01, 0.1]
      subsample:
        values: [0.8, 1.0]
      colsample_bytree:
        values: [0.8, 1.0]
      reg_alpha:
        dist: loguniform
        low: 1e-6
        high: 1.0
      reg_lambda:
        dist: loguniform
        low: 1e-6
        high: 1.0
      max_bin:
        values: [63, 127, 255]
      bagging_freq:
        values: [1, 10]
      verbosity:
        values: [-1]
    lgbm_gbdt_huber:
      n_estimators:
        values: [600, 900, 1200]
      learning_rate:
        values: [0.005, 0.01, 0.05, 0.1]
      max_depth:
        values: [-1, 2, 3]
      num_leaves:
        values: [10, 15, 30, 45]
      min_child_samples:
        values: [150, 200, 300]
      min_split_gain:
        values: [0.0, 0.05, 0.1]
      min_child_weight:
        values: [0.001, 0.01, 0.1]
      subsample:
        values: [0.8, 1.0]
      colsample_bytree:
        values: [0.8, 1.0]
      reg_alpha:
        dist: loguniform
        low: 0.000001
        high: 1.0
      reg_lambda:
        dist: loguniform
        low: 0.000001
        high: 1.0
      max_bin:
        values: [63, 127, 255]
      bagging_freq:
        values: [1, 10]
      verbosity:
        values: [-1]
      huber_delta:
        values: [0.5, 1.0, 2.0]
    lgbm_dart:
      n_estimators:
        values: [600, 900, 1200]
      learning_rate:
        values: [0.005, 0.01, 0.02, 0.03]
      max_depth:
        values: [-1, 3, 4]
      num_leaves:
        values: [31, 63, 127]
      min_child_samples:
        values: [50, 100, 200]
      min_split_gain:
        values: [0.0, 0.05, 0.1]
      min_child_weight:
        values: [0.001, 0.01, 0.1]
      subsample:
        values: [0.8, 1.0]
      colsample_bytree:
        values: [0.8, 1.0]
      reg_alpha:
        dist: loguniform
        low: 0.000001
        high: 1.0
      reg_lambda:
        dist: loguniform
        low: 0.000001
        high: 1.0
      drop_rate:
        values: [0.10, 0.15, 0.20]
      skip_drop:
        values: [0.10, 0.25]
      max_drop:
        values: [10, 50]
      max_bin:
        values: [63, 127, 255]
      bagging_freq:
        values: [1, 10]
      verbosity:
        values: [-1]
    lgbm_dart_huber:
      n_estimators:
        values: [600, 900, 1200]
      learning_rate:
        values: [0.005, 0.01, 0.02, 0.03]
      max_depth:
        values: [-1, 3, 4]
      num_leaves:
        values: [15, 31, 63, 127]
      min_child_samples:
        values: [100, 150, 200, 300]
      min_split_gain:
        values: [0.0, 0.05, 0.1]
      min_child_weight:
        values: [0.001, 0.01, 0.1]
      subsample:
        values: [0.8, 1.0]
      colsample_bytree:
        values: [0.8, 1.0]
      reg_alpha:
        dist: loguniform
        low: 0.000001
        high: 1.0
      reg_lambda:
        dist: loguniform
        low: 0.000001
        high: 1.0
      drop_rate:
        values: [0.10, 0.15, 0.20]
      skip_drop:
        values: [0.10, 0.25]
      max_drop:
        values: [10, 50]
      max_bin:
        values: [63, 127, 255]
      bagging_freq:
        values: [1, 10]
      verbosity:
        values: [-1]
      huber_delta:
        values: [0.5, 1.0, 2.0]
    # Huber variants of LightGBM use their defaults from models.py
    # (including huber_delta), so we do not override their spaces here.
    ffn:
      n_hidden_layers:
        values: [1, 2]
      hidden_width:
        values: [32, 64, 128]
      width_shrink:
        values: [1.0, 0.5]
      activation:
        values: ["relu", "tanh", "gelu"]
      dropout:
        dist: uniform
        low: 0.1
        high: 0.5
      learning_rate:
        values: [0.001, 0.0005, 0.0003, 0.0001]
      weight_decay:
        dist: loguniform
        low: 1e-6
        high: 1e-3
      max_epochs:
        values: [80, 160]
      batch_size:
        values: [128, 256, 512]
      patience:
        values: [8, 16]
      val_split:
        values: [0.1]
      seed:
        values: [0]
      device:
        values: ["auto"]
      loss:
        values: ["mse"]
      huber_delta:
        values: [0.5, 1.0, 2.0]


explain:
  shap:
    enabled: true
    models: ["best"]
    sample_rows: 500
    background_kmeans: 20
    check_additivity: false
    per_member: true

portfolio:
  n_bins: 3

logging:
  level: INFO
  show_progress: true
